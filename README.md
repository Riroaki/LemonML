# ğŸ‹LemonğŸ‹

> **åŸºäºnumpyçš„åŸºæœ¬æœºå™¨å­¦ä¹ ç®—æ³•åº“**

æœ¬é¡¹ç›®èµ·æºäºZJU2019å¹´æ˜¥å¤å­¦æœŸçš„ã€Šæ•°æ®æŒ–æ˜å¯¼è®ºã€‹è¯¾ç¨‹ä½œä¸šï¼Œä»£ç ä¸Šä¼ åœ¨æˆ‘çš„[å¦ä¸€ä¸ªé¡¹ç›®](https://github.com/Riroaki/DataMining-ZJU-19-Summer)ã€‚

è¿‘æœŸä¹Ÿåœ¨ä¸ªäººåšå®¢çš„ã€Šæœºå™¨å­¦ä¸åŠ¨äº†ã€‹[ä¸“æ ]([https://riroaki.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B8%8D%E5%8A%A8%E4%BA%86/](https://riroaki.github.io/categories/æœºå™¨å­¦ä¸åŠ¨äº†/))åŒæ­¥æ›´æ–°é¡¹ç›®ä¸­æ¶‰åŠçš„æœºå™¨å­¦ä¹ ç®—æ³•ç³»åˆ—å†…å®¹ã€‚

## ç›®æ ‡ï¼š

- æ¸…æ™°æ˜“æ‡‚çš„ä»£ç å’Œæ³¨é‡Š
- ç®€å•æ˜“ç”¨çš„API
- å…¨é¢çš„æœºå™¨å­¦ä¹ ç®—æ³•

## Structure

### æœ‰ç›‘ç£Supervised

- çº¿æ€§ç±»
  - çº¿æ€§å›å½’ï¼ˆåŸºäºæ¢¯åº¦/åŸºäºnormal equationï¼‰
  - é€»è¾‘å›å½’åˆ†ç±»
  - æ„ŸçŸ¥æœºåˆ†ç±»
  - SVMåˆ†ç±»
- éçº¿æ€§ç±»
  - è´å¶æ–¯åˆ†ç±»
  - kè¿‘é‚»åˆ†ç±»
  - å†³ç­–æ ‘åˆ†ç±»

### æ— ç›‘ç£Unsupervised

- K-Meansèšç±»
- Spectralèšç±»
- PCAä¸»æˆåˆ†åˆ†æ
- â€¦â€¦

### èšåˆEmsemble

- Bagging
  - Random Forest
- Adaboost
- â€¦â€¦

### ç¥ç»ç½‘ç»œNN

- â€¦â€¦

### å·¥å…·Utils

- batchæ‰¹é‡åˆ†å‰²
- scalingç¼©æ”¾ï¼ˆmin-max/mean/standardizatioin/unitï¼‰
- cross validationäº¤å‰éªŒè¯ï¼ˆK-fold/Leave-one-outï¼‰

## API

### SupervisedModel

- Class
  - `LinearRegression`
  - `LogisticRegression`
  - `Perceptron`
  - `SVM`
  - `KNearest`
  - `DecisionTree`
  - `...`
- Methods
  - `fit(x: np.ndarray, y: np.ndarray, **kwargs) -> np.ndarray`
  - `predict(x: np.ndarray, **kwargs) -> np.ndarray`
  - `evaluate(x: np.ndarray, y: np.ndarray, **kwargs) -> tuple`
  - `dump(dump_file: str) -> None`
  - `load(dump_file: str) -> None`

### UnsupervisedModel

- Class
  - `KMeans`
  - `Spectral`
  - `PCA`
  - `...`
- Methods
  - `Clustering(x: np.ndarray, **kwargs) -> np.ndarray`

### Utils

- `batch`
  - `batch(data: np.ndarray, y: np.ndarray, size: int, shuffle: bool = False) -> tuple`
- `cross_validate`
  - `k_fold(data: np.ndarray, y: np.ndarray, k: int, fit_func: callable, eval_func: callable, shuffle: bool = True) -> tuple`
  - `leave_one_out(data: np.ndarray, y: np.ndarray, fit_func: callable, eval_func: callable, shuffle: bool = True) -> tuple`
- `make_data`
  - `linear(n: int, dim: int, rand_bound: float = 10., noisy: bool = False) -> tuple`
  - `logistic(n: int, dim: int, rand_bound: float = 10., noisy: bool = False) -> tuple`
  - `perceptron(n: int, dim: int, rand_bound: float = 10., noisy: bool = False) -> tuple`
  - `svm(n: int, dim: int, rand_bound: float = 10., noisy: bool = False) -> tuple`
  - `...`
- `scaling`
  - `std(data: np.ndarray) -> None`
  - `minmax(data: np.ndarray) -> None`
  - `mean(data: np.ndarray) -> None`
  - `unit(data: np.ndarray) -> None`

## Timeline

- 2019.6.12
  - [x] Linear Regression
  - [x] Logistic Regression
  - [x] Perceptron
  - [x] utils.scaling / batch / cross_validate
- 6.13
  - [x] Support Vector Machine
  - [x] K-Nearest-Neighbor
  - [x] test script
- 6.15
  - [x] Bayes
- 6.16
  - [x] K-Means
- 6.19
  - [x] Spectral
  - [x] Principle Component Analysis

- 6.24
  - [x] Decision Tree
- 6.27
  - [ ] Add multi-classifier support for binary-classifiers
  - [ ] Add Ridge and Lasso support for linear classifiers
  - [ ] Start Emsemble and Neural Network

## TODO

- [ ] å……Ridgeå’ŒLassoç›¸å…³å†…å®¹ï¼ˆé¢„è®¡ä½¿ç”¨è£…é¥°å™¨ï¼‰
- [ ] ç›®å‰éƒ¨åˆ†åˆ†ç±»ç®—æ³•ä¸ºäºŒåˆ†ç±»ï¼Œéœ€è¦è¡¥å……å¢åŠ å¤šåˆ†ç±»å®ç°
- [ ] å¢åŠ å¤šç§æŸå¤±å‡½æ•°åŠå¯¹åº”æ¢¯åº¦è®¡ç®—æ–¹æ³•å®ç°
- [ ] å¢åŠ boostç­‰ensembleæ–¹æ³•å®ç°ï¼ˆåŸºæœ¬ç›®æ ‡ï¼šrandom forestï¼‰
- [ ] å¯¹çº¿æ€§æ¨¡å‹ï¼Œå¢åŠ å¤šç§å­¦ä¹ ç‡ä¼˜åŒ–æ–¹å¼ï¼Œå¦‚adagradç­‰

- [ ] å¢åŠ ç®—æ³•å¯è§†åŒ–